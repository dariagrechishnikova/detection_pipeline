# -*- coding: utf-8 -*-
"""trainer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qXXTYDRG_LV1tCM2IiX2B1dQOmsftMRn
"""

import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TensorBoard
from tensorflow.keras.optimizers import Adam
from dataclasses import dataclass
from runner import *
from data_provider import *
from custom_models import *
from custom_metrics import *
from custom_losses import *
import os
import random
from tensorflow.keras.layers.experimental import preprocessing 
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import albumentations as A
import numpy as np
from keras.models import Model
from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D
from keras.layers import BatchNormalization
from keras.layers.core import SpatialDropout2D, Activation
from tensorflow.keras.applications import EfficientNetB0
from keras import backend as K
from keras.layers.merge import concatenate
from keras.utils.data_utils import get_file
from tensorflow import keras
import cv2
import datetime
from pathlib import Path
from settings import *
import wandb
from wandb.keras import WandbCallback




class trainer_detection():
  def __init__(self, input_model, input_optimizer, input_loss, input_metrics,
               input_batch_size, input_epochs, input_callbacks, config_dict, input_log_name,
               input_net_info_path, input_model_checkpoints_path):
      self.current_model = input_model
      self.batch_size = input_batch_size
      self.optimizer = input_optimizer
      self.loss = input_loss
      self.metrics = input_metrics
      self.epochs = input_epochs
      self.custom_callbacks = input_callbacks
      self.config_dict = config_dict
      self.log_name = input_log_name
      self.net_info_path =  input_net_info_path
      self.model_checkpoints_path = input_model_checkpoints_path
      
  
  class lr_to_csvlogger(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs):
      logs['learning_rate'] = self.model.optimizer._decayed_lr('float32').numpy()

  def setup_wandb_tracking(self):
    wandb.login()
    run = wandb.init(config=self.config_dict)
    

  def train(self, train_data, val_data):
    self.setup_wandb_tracking()
    self.current_model.compile(optimizer=self.optimizer, loss=self.loss, metrics = self.metrics)
    history = self.current_model.fit(train_data,
    validation_data=val_data,
    epochs=self.epochs,
    callbacks=[self.lr_to_csvlogger(), CSVLogger(Path(self.net_info_path, f"model_{self.log_name}.csv")), 
          ModelCheckpoint(monitor='val_loss', filepath=Path(self.model_checkpoints_path, f"model_{self.log_name}.hdf5"), save_best_only=True),
          TensorBoard(log_dir=Path(self.net_info_path, 'tensorboard_logs'), histogram_freq=1), WandbCallback()] + self.custom_callbacks)
    return

